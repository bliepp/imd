====== Usage ======

After having [[guide_compiling|compiled]] //IMD// with the suitable [[guide_options|options]], the simulation is started as follows:

<code bash>imd_program [-r N] -p parameter_file</code>

The name of the //IMD// program is the same as the compilation target. If the parameter ''-r N'' is present, where ''N'' is an integer number, the simulation is restarted from the checkpoint number ''N''. The [[guide_parameter_file|parameter file]] contains all further [[guide_parameters|parameters]] describing the simulation.

===== Parallel execution =====

//IMD// supports two parallelisation methods, OpenMP and MPI, which can also be combined. If OpenMP is used, the number of threads in each process has to specified by setting the envirent variable ''OMP_NUM_TREADS''. If MPI is used, the number of MPI processes must be stated at the program start. This is usually done with a (system dependent) startup script like the following ones:

<code bash>mpirun -np P imd_program [-r N] -p parameter_file   # MPICH
mpprun -n  P imd_program [-r N] -p parameter_file   # Cray T3E</code>
    
Here, ''P'' is the number of MPI processes. If ''P'' does not correspond to the number of processes specified by the parameter ''cpu_dim'' in the parameter file or if ''cpu_dim'' is missing, //IMD// chooses an appropriate cpu grid by factorizing ''P'' evenly and by adhering to the size distribution of ''cpu_dim'' (if given).

If MPI and OpenMP are combined, one usually wants to run one MPI process per computer node, with the number of OpenMP threads equal to the number of CPUs per node. Care must be taken that exactly one process is allocated on each node. In a pure MPI scheme, the number of processes ''P'' should be equal to the total number of CPUs available. Which parallelisation method is faster has to be checked case by case.
